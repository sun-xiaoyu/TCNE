{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri May  3 18:07:45 2019. At 0 s, wtf\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Fri May  3 18:07:45 2019. At 0 s, wtf\\n'"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def Time(s):\n",
    "    str = time.asctime(time.localtime())+\". At %d s, \" % (time.time() - start) + s + '\\n'\n",
    "    print(str,end='')\n",
    "    return str\n",
    "start = time.time()\n",
    "Time(\"xx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## An example of TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    'This is the first document.',\n",
    "    'This document is the second document.',\n",
    "    'And this is the third one.',\n",
    "    'Is this the first document?',\n",
    "]\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "print(vectorizer.get_feature_names())\n",
    "\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.         0.46979139 0.58028582 0.38408524 0.         0.\n",
      "  0.38408524 0.         0.38408524]\n",
      " [0.         0.6876236  0.         0.28108867 0.         0.53864762\n",
      "  0.28108867 0.         0.28108867]\n",
      " [0.51184851 0.         0.         0.26710379 0.51184851 0.\n",
      "  0.26710379 0.51184851 0.26710379]\n",
      " [0.         0.46979139 0.58028582 0.38408524 0.         0.\n",
      "  0.38408524 0.         0.38408524]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.        , 0.6876236 , 0.        , 0.28108867, 0.        ,\n",
       "        0.53864762, 0.28108867, 0.        , 0.28108867]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(X.toarray())\n",
    "X[1].toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a, b =corpus[1].split(' ',1)\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting feature matrix of TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tf_idf_feature_matrix(corpus, combining, use_idf = True):\n",
    "    documents = []\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    if combining == \"avg\":\n",
    "        with open(\"data/%s/%s-train-stemmed.txt\" % (corpus, corpus),'r') as f:\n",
    "            collection = f.readlines()\n",
    "            documents = []\n",
    "            n = len(collection)\n",
    "            y_train = [\"\"]*n\n",
    "            for i, a_line in enumerate(collection):\n",
    "                words = a_line.split()\n",
    "                y_train[i] = words[0]\n",
    "                doc = ' '.join(words[1:])\n",
    "                '''\n",
    "                split = a_line.split(' ',1)\n",
    "                if (len(split)==2):\n",
    "                    y_train[i]= split[0]\n",
    "                    doc = split[1]\n",
    "                else:\n",
    "                    y_train[i] = split[0]\n",
    "                    doc = \"\"\n",
    "                '''\n",
    "                documents.append(doc)\n",
    "            y_train = le.fit_transform(y_train)\n",
    "            vectorizer = TfidfVectorizer(use_idf = use_idf, stop_words = None)\n",
    "            x_train = vectorizer.fit_transform(documents)\n",
    "            print(x_train.shape)\n",
    "        with open(\"data/%s/%s-test-stemmed.txt\" % (corpus, corpus),'r') as f:\n",
    "            collection = f.readlines()\n",
    "            documents = []\n",
    "            n = len(collection)\n",
    "            y_test = [\"\"]*n\n",
    "            for i, a_line in enumerate(collection):\n",
    "                words = a_line.split()\n",
    "                y_test[i] = words[0]\n",
    "                doc = ' '.join(words[1:])\n",
    "                documents.append(doc)\n",
    "            x_test = vectorizer.transform(documents)\n",
    "            y_test = le.transform(y_test)\n",
    "#         x_train[np.isnan(x_train)] = 0\n",
    "#         x_test[np.isnan(x_test)] = 0\n",
    "        return x_train, y_train, x_test, y_test, le.classes_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline for webKB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2803, 7287)\n"
     ]
    }
   ],
   "source": [
    "corpus = \"webkb\"\n",
    "X_train, y_train, X_test, y_test, categories = get_tf_idf_feature_matrix(corpus, \"avg\", False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the classifier...\n",
      "Tue May  7 16:26:38 2019. At 339532 s, all done.\n",
      "\n",
      "Features shape:(2803, 7287)\n",
      "Accuracy in training set:0.9864430966821263\n",
      "Macro:(0.9880176414276283, 0.9840598618001616, 0.9859613664862547, None)\n",
      "Micro:(0.9864430966821263, 0.9864430966821263, 0.9864430966821263, None)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      course     0.9825    0.9952    0.9888       620\n",
      "     faculty     0.9946    0.9733    0.9838       750\n",
      "     project     0.9939    0.9732    0.9835       336\n",
      "     student     0.9811    0.9945    0.9878      1097\n",
      "\n",
      "   micro avg     0.9864    0.9864    0.9864      2803\n",
      "   macro avg     0.9880    0.9841    0.9860      2803\n",
      "weighted avg     0.9865    0.9864    0.9864      2803\n",
      "\n",
      "Accuracy in testing set:0.9097421203438395\n",
      "Macro test:(0.90284653330314, 0.8992648085627213, 0.9009930564226343, None)\n",
      "Micro test:(0.9097421203438395, 0.9097421203438395, 0.9097421203438396, None)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      course     0.9579    0.9548    0.9564       310\n",
      "     faculty     0.8816    0.8957    0.8886       374\n",
      "     project     0.8528    0.8274    0.8399       168\n",
      "     student     0.9191    0.9191    0.9191       544\n",
      "\n",
      "   micro avg     0.9097    0.9097    0.9097      1396\n",
      "   macro avg     0.9028    0.8993    0.9010      1396\n",
      "weighted avg     0.9097    0.9097    0.9097      1396\n",
      "Tue May  7 16:26:38 2019. At 339532 s, all done.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm, metrics\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "svc = svm.LinearSVC()\n",
    "parameters = [{'C': [0.01, 0.1, 1, 10, 100, 1000]}]\n",
    "clf = GridSearchCV(svc, parameters, n_jobs=-1, cv=10)\n",
    "print(\"Training the classifier...\")\n",
    "clf.fit(X_train,y_train)\n",
    "forest = clf.fit(X_train, y_train)\n",
    "report = \"\"\n",
    "pred_train = forest.predict(X_train)\n",
    "\n",
    "# training score\n",
    "score = accuracy_score(y_train, pred_train)\n",
    "# score = metrics.f1_score(y_test, pred_test, pos_label=list(set(y_test)))\n",
    "acc = \"Accuracy in training set:\" + str(score)\n",
    "mac = \"Macro:\" + str(metrics.precision_recall_fscore_support(y_train, pred_train, average='macro'))\n",
    "mic = \"Micro:\" + str(metrics.precision_recall_fscore_support(y_train, pred_train, average='micro'))\n",
    "met = metrics.classification_report(y_train, pred_train, target_names=categories, digits=4)\n",
    "\n",
    "report += \"\\nFeatures shape:\" + str(X_train.shape) + \"\\n\"\n",
    "report += '\\n'.join([acc, mac, mic, met])\n",
    "\n",
    "pred_test = forest.predict(X_test)\n",
    "\n",
    "# testing score\n",
    "score = accuracy_score(y_test, pred_test)\n",
    "# score = metrics.f1_score(y_test, pred_test, pos_label=list(set(y_test)))\n",
    "acc = \"Accuracy in testing set:\" + str(score)\n",
    "mac = \"Macro test:\" + str(metrics.precision_recall_fscore_support(y_test, pred_test, average='macro'))\n",
    "mic = \"Micro test:\" + str(metrics.precision_recall_fscore_support(y_test, pred_test, average='micro'))\n",
    "met = metrics.classification_report(y_test, pred_test, target_names=categories, digits=4)\n",
    "report += '\\n'+'\\n'.join([acc, mac, mic, met])\n",
    "report += Time(\"all done.\")\n",
    "\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2803, 7287)\n"
     ]
    }
   ],
   "source": [
    "corpus = \"webkb\"\n",
    "X_train, y_train, X_test, y_test, categories = get_tf_idf_feature_matrix(corpus, \"avg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the classifier...\n",
      "Fri May  3 18:18:53 2019. At 668 s, all done.\n",
      "\n",
      "Features shape:(2803, 7287)\n",
      "Accuracy in training set:0.9946485907955762\n",
      "Macro:(0.9953956947920779, 0.9939787352953944, 0.9946613396980591, None)\n",
      "Micro:(0.9946485907955762, 0.9946485907955762, 0.9946485907955762, None)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      course     0.9888    1.0000    0.9944       620\n",
      "     faculty     1.0000    0.9867    0.9933       750\n",
      "     project     1.0000    0.9911    0.9955       336\n",
      "     student     0.9927    0.9982    0.9955      1097\n",
      "\n",
      "   micro avg     0.9946    0.9946    0.9946      2803\n",
      "   macro avg     0.9954    0.9940    0.9947      2803\n",
      "weighted avg     0.9947    0.9946    0.9946      2803\n",
      "\n",
      "Accuracy in testing set:0.8982808022922636\n",
      "Macro test:(0.8936884490291144, 0.8806370238711321, 0.8866196973985003, None)\n",
      "Micro test:(0.8982808022922636, 0.8982808022922636, 0.8982808022922636, None)\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      course     0.9486    0.9516    0.9501       310\n",
      "     faculty     0.8697    0.8743    0.8720       374\n",
      "     project     0.8553    0.7738    0.8125       168\n",
      "     student     0.9013    0.9228    0.9119       544\n",
      "\n",
      "   micro avg     0.8983    0.8983    0.8983      1396\n",
      "   macro avg     0.8937    0.8806    0.8866      1396\n",
      "weighted avg     0.8978    0.8983    0.8977      1396\n",
      "Fri May  3 18:18:53 2019. At 668 s, all done.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm, metrics\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "svc = svm.LinearSVC()\n",
    "parameters = [{'C': [0.01, 0.1, 1, 10, 100, 1000]}]\n",
    "clf = GridSearchCV(svc, parameters, n_jobs=-1, cv=10)\n",
    "print(\"Training the classifier...\")\n",
    "clf.fit(X_train,y_train)\n",
    "forest = clf.fit(X_train, y_train)\n",
    "report = \"\"\n",
    "pred_train = forest.predict(X_train)\n",
    "\n",
    "# training score\n",
    "score = accuracy_score(y_train, pred_train)\n",
    "# score = metrics.f1_score(y_test, pred_test, pos_label=list(set(y_test)))\n",
    "acc = \"Accuracy in training set:\" + str(score)\n",
    "mac = \"Macro:\" + str(metrics.precision_recall_fscore_support(y_train, pred_train, average='macro'))\n",
    "mic = \"Micro:\" + str(metrics.precision_recall_fscore_support(y_train, pred_train, average='micro'))\n",
    "met = metrics.classification_report(y_train, pred_train, target_names=categories, digits=4)\n",
    "\n",
    "report += \"\\nFeatures shape:\" + str(X_train.shape) + \"\\n\"\n",
    "report += '\\n'.join([acc, mac, mic, met])\n",
    "\n",
    "pred_test = forest.predict(X_test)\n",
    "\n",
    "# testing score\n",
    "score = accuracy_score(y_test, pred_test)\n",
    "# score = metrics.f1_score(y_test, pred_test, pos_label=list(set(y_test)))\n",
    "acc = \"Accuracy in testing set:\" + str(score)\n",
    "mac = \"Macro test:\" + str(metrics.precision_recall_fscore_support(y_test, pred_test, average='macro'))\n",
    "mic = \"Micro test:\" + str(metrics.precision_recall_fscore_support(y_test, pred_test, average='micro'))\n",
    "met = metrics.classification_report(y_test, pred_test, target_names=categories, digits=4)\n",
    "report += '\\n'+'\\n'.join([acc, mac, mic, met])\n",
    "report += Time(\"all done.\")\n",
    "\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF + TCNE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2803, 7287)\n"
     ]
    }
   ],
   "source": [
    "corpus = \"webkb\"\n",
    "X_train, y_train, X_test, y_test, categories = get_tf_idf_feature_matrix(corpus, \"avg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/sunxiaoyu/PycharmProjects/TCNE\n",
      "Reading...\n",
      "Name: \n",
      "Type: DiGraph\n",
      "Number of nodes: 7287\n",
      "Number of edges: 660051\n",
      "Average in degree:  90.5793\n",
      "Average out degree:  90.5793\n",
      "Embedding node2vec read from fle.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sunxiaoyu/.local/lib/python3.6/site-packages/numpy/core/fromnumeric.py:3118: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/home/sunxiaoyu/.local/lib/python3.6/site-packages/numpy/core/_methods.py:85: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ],
   "source": [
    "from src.feature_generation import get_feature_matrix\n",
    "from src.build_graph import build_col_graph_from_train\n",
    "from src.graph_to_embedding import graph_to_embedding\n",
    "from src.graph import Graph\n",
    "combining = 'avg'\n",
    "corpus = \"webkb\"\n",
    "method = 'node2vec'\n",
    "p = 0.3\n",
    "q = 1\n",
    "emb_dim = 128\n",
    "graph_filepath = build_col_graph_from_train(corpus)\n",
    "g = Graph()\n",
    "g.read_edgelist(filename=graph_filepath, weighted=True, directed=False)\n",
    "emb = graph_to_embedding(g, method, corpus, emb_dim, p, q)\n",
    "X_train_1, y_train, X_test_1, y_test, categories = get_feature_matrix(emb, corpus, combining)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2803, 128)\n",
      "(1396, 128)\n",
      "(2803, 7287)\n",
      "(1396, 7287)\n"
     ]
    }
   ],
   "source": [
    "print(X_train_1.shape)\n",
    "print(X_test_1.shape)\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "zero-dimensional arrays cannot be concatenated",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-129-3e6a6bc67fd2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train_1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m: zero-dimensional arrays cannot be concatenated"
     ]
    }
   ],
   "source": [
    "np.concatenate((X_train, X_train_1),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "zero-dimensional arrays cannot be concatenated",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-130-108c4c4d7406>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mX_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcatenate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test_1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m: zero-dimensional arrays cannot be concatenated"
     ]
    }
   ],
   "source": [
    "X_test = np.concatenate((X_test, X_test_1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm, metrics\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "svc = svm.LinearSVC()\n",
    "parameters = [{'C': [0.01, 0.1, 1, 10, 100, 1000]}]\n",
    "clf = GridSearchCV(svc, parameters, n_jobs=-1, cv=10)\n",
    "print(\"Training the classifier...\")\n",
    "clf.fit(X_train,y_train)\n",
    "forest = clf.fit(X_train, y_train)\n",
    "report = \"\"\n",
    "pred_train = forest.predict(X_train)\n",
    "\n",
    "# training score\n",
    "score = accuracy_score(y_train, pred_train)\n",
    "# score = metrics.f1_score(y_test, pred_test, pos_label=list(set(y_test)))\n",
    "acc = \"Accuracy in training set:\" + str(score)\n",
    "mac = \"Macro:\" + str(metrics.precision_recall_fscore_support(y_train, pred_train, average='macro'))\n",
    "mic = \"Micro:\" + str(metrics.precision_recall_fscore_support(y_train, pred_train, average='micro'))\n",
    "met = metrics.classification_report(y_train, pred_train, target_names=categories, digits=4)\n",
    "\n",
    "report += \"\\nFeatures shape:\" + str(X_train.shape) + \"\\n\"\n",
    "report += '\\n'.join([acc, mac, mic, met])\n",
    "\n",
    "pred_test = forest.predict(X_test)\n",
    "\n",
    "# testing score\n",
    "score = accuracy_score(y_test, pred_test)\n",
    "# score = metrics.f1_score(y_test, pred_test, pos_label=list(set(y_test)))\n",
    "acc = \"Accuracy in testing set:\" + str(score)\n",
    "mac = \"Macro test:\" + str(metrics.precision_recall_fscore_support(y_test, pred_test, average='macro'))\n",
    "mic = \"Micro test:\" + str(metrics.precision_recall_fscore_support(y_test, pred_test, average='micro'))\n",
    "met = metrics.classification_report(y_test, pred_test, target_names=categories, digits=4)\n",
    "report += '\\n'+'\\n'.join([acc, mac, mic, met])\n",
    "report += Time(\"all done.\")\n",
    "\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baseline for 20NG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11293, 54577)\n"
     ]
    }
   ],
   "source": [
    "corpus = \"20NG\"\n",
    "X_train, y_train, X_test, y_test, categories = get_tf_idf_feature_matrix(corpus, \"avg\", False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the classifier...\n",
      "Sat May  4 18:03:41 2019. At 86155 s, all done.\n",
      "\n",
      "Features shape:(11293, 54577)\n",
      "Accuracy in training set:0.9936243690781901\n",
      "Macro:(0.9937145607536706, 0.9934610435105654, 0.9935771698132836, None)\n",
      "Micro:(0.9936243690781901, 0.9936243690781901, 0.9936243690781901, None)\n",
      "                          precision    recall  f1-score   support\n",
      "\n",
      "             alt.atheism     0.9979    0.9896    0.9937       480\n",
      "           comp.graphics     0.9948    0.9897    0.9923       584\n",
      " comp.os.ms-windows.misc     0.9843    0.9878    0.9860       572\n",
      "comp.sys.ibm.pc.hardware     0.9762    0.9729    0.9745       590\n",
      "   comp.sys.mac.hardware     0.9931    0.9896    0.9913       578\n",
      "          comp.windows.x     0.9949    0.9966    0.9958       593\n",
      "            misc.forsale     0.9681    0.9863    0.9771       585\n",
      "               rec.autos     0.9966    0.9949    0.9958       594\n",
      "         rec.motorcycles     1.0000    0.9950    0.9975       598\n",
      "      rec.sport.baseball     1.0000    1.0000    1.0000       597\n",
      "        rec.sport.hockey     0.9917    1.0000    0.9959       600\n",
      "               sci.crypt     0.9983    1.0000    0.9992       595\n",
      "         sci.electronics     1.0000    0.9898    0.9949       591\n",
      "                 sci.med     1.0000    0.9983    0.9992       594\n",
      "               sci.space     1.0000    0.9983    0.9992       593\n",
      "  soc.religion.christian     0.9917    1.0000    0.9958       598\n",
      "      talk.politics.guns     0.9945    1.0000    0.9973       545\n",
      "   talk.politics.mideast     1.0000    1.0000    1.0000       564\n",
      "      talk.politics.misc     1.0000    0.9935    0.9968       465\n",
      "      talk.religion.misc     0.9920    0.9867    0.9894       377\n",
      "\n",
      "               micro avg     0.9936    0.9936    0.9936     11293\n",
      "               macro avg     0.9937    0.9935    0.9936     11293\n",
      "            weighted avg     0.9937    0.9936    0.9936     11293\n",
      "\n",
      "Accuracy in testing set:0.820114255347416\n",
      "Macro test:(0.816384402476228, 0.8109989624750196, 0.8112619177213217, None)\n",
      "Micro test:(0.820114255347416, 0.820114255347416, 0.8201142553474161, None)\n",
      "                          precision    recall  f1-score   support\n",
      "\n",
      "             alt.atheism     0.7169    0.7304    0.7236       319\n",
      "           comp.graphics     0.7620    0.7738    0.7679       389\n",
      " comp.os.ms-windows.misc     0.7405    0.6972    0.7182       393\n",
      "comp.sys.ibm.pc.hardware     0.6905    0.7398    0.7143       392\n",
      "   comp.sys.mac.hardware     0.7524    0.8130    0.7815       385\n",
      "          comp.windows.x     0.8746    0.7474    0.8061       392\n",
      "            misc.forsale     0.8225    0.8795    0.8501       390\n",
      "               rec.autos     0.9108    0.8785    0.8943       395\n",
      "         rec.motorcycles     0.9322    0.9322    0.9322       398\n",
      "      rec.sport.baseball     0.9246    0.9270    0.9258       397\n",
      "        rec.sport.hockey     0.9438    0.9674    0.9554       399\n",
      "               sci.crypt     0.9113    0.9343    0.9227       396\n",
      "         sci.electronics     0.7374    0.7430    0.7402       393\n",
      "                 sci.med     0.8906    0.8636    0.8769       396\n",
      "               sci.space     0.9008    0.8985    0.8996       394\n",
      "  soc.religion.christian     0.7739    0.8945    0.8298       398\n",
      "      talk.politics.guns     0.7106    0.9038    0.7956       364\n",
      "   talk.politics.mideast     0.9744    0.8112    0.8853       376\n",
      "      talk.politics.misc     0.7406    0.5710    0.6448       310\n",
      "      talk.religion.misc     0.6172    0.5139    0.5609       251\n",
      "\n",
      "               micro avg     0.8201    0.8201    0.8201      7527\n",
      "               macro avg     0.8164    0.8110    0.8113      7527\n",
      "            weighted avg     0.8224    0.8201    0.8190      7527\n",
      "Sat May  4 18:03:41 2019. At 86155 s, all done.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "svc = svm.LinearSVC()\n",
    "parameters = [{'C': [0.01, 0.1, 1, 10, 100, 1000]}]\n",
    "clf = GridSearchCV(svc, parameters, n_jobs=-1, cv=10)\n",
    "print(\"Training the classifier...\")\n",
    "clf.fit(X_train,y_train)\n",
    "forest = clf.fit(X_train, y_train)\n",
    "report = \"\"\n",
    "pred_train = forest.predict(X_train)\n",
    "\n",
    "# training score\n",
    "score = accuracy_score(y_train, pred_train)\n",
    "# score = metrics.f1_score(y_test, pred_test, pos_label=list(set(y_test)))\n",
    "acc = \"Accuracy in training set:\" + str(score)\n",
    "mac = \"Macro:\" + str(metrics.precision_recall_fscore_support(y_train, pred_train, average='macro'))\n",
    "mic = \"Micro:\" + str(metrics.precision_recall_fscore_support(y_train, pred_train, average='micro'))\n",
    "met = metrics.classification_report(y_train, pred_train, target_names=categories, digits=4)\n",
    "\n",
    "report += \"\\nFeatures shape:\" + str(X_train.shape) + \"\\n\"\n",
    "report += '\\n'.join([acc, mac, mic, met])\n",
    "\n",
    "pred_test = forest.predict(X_test)\n",
    "\n",
    "# testing score\n",
    "score = accuracy_score(y_test, pred_test)\n",
    "# score = metrics.f1_score(y_test, pred_test, pos_label=list(set(y_test)))\n",
    "acc = \"Accuracy in testing set:\" + str(score)\n",
    "mac = \"Macro test:\" + str(metrics.precision_recall_fscore_support(y_test, pred_test, average='macro'))\n",
    "mic = \"Micro test:\" + str(metrics.precision_recall_fscore_support(y_test, pred_test, average='micro'))\n",
    "met = metrics.classification_report(y_test, pred_test, target_names=categories, digits=4)\n",
    "report += '\\n'+'\\n'.join([acc, mac, mic, met])\n",
    "report += Time(\"all done.\")\n",
    "\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF_IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11293, 54577)\n"
     ]
    }
   ],
   "source": [
    "corpus = \"20NG\"\n",
    "X_train, y_train, X_test, y_test, categories = get_tf_idf_feature_matrix(corpus, \"avg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the classifier...\n",
      "Sat May  4 17:29:26 2019. At 84101 s, all done.\n",
      "\n",
      "Features shape:(11293, 54577)\n",
      "Accuracy in training set:0.9982289914106084\n",
      "Macro:(0.9983048253837111, 0.998260848687322, 0.9982773536223088, None)\n",
      "Micro:(0.9982289914106084, 0.9982289914106084, 0.9982289914106084, None)\n",
      "                          precision    recall  f1-score   support\n",
      "\n",
      "             alt.atheism     1.0000    0.9979    0.9990       480\n",
      "           comp.graphics     0.9983    0.9932    0.9957       584\n",
      " comp.os.ms-windows.misc     0.9983    1.0000    0.9991       572\n",
      "comp.sys.ibm.pc.hardware     0.9916    0.9949    0.9932       590\n",
      "   comp.sys.mac.hardware     1.0000    0.9965    0.9983       578\n",
      "          comp.windows.x     1.0000    1.0000    1.0000       593\n",
      "            misc.forsale     0.9832    1.0000    0.9915       585\n",
      "               rec.autos     0.9983    0.9983    0.9983       594\n",
      "         rec.motorcycles     1.0000    0.9967    0.9983       598\n",
      "      rec.sport.baseball     1.0000    1.0000    1.0000       597\n",
      "        rec.sport.hockey     1.0000    0.9983    0.9992       600\n",
      "               sci.crypt     1.0000    1.0000    1.0000       595\n",
      "         sci.electronics     1.0000    0.9915    0.9958       591\n",
      "                 sci.med     1.0000    1.0000    1.0000       594\n",
      "               sci.space     1.0000    1.0000    1.0000       593\n",
      "  soc.religion.christian     0.9983    1.0000    0.9992       598\n",
      "      talk.politics.guns     0.9982    1.0000    0.9991       545\n",
      "   talk.politics.mideast     1.0000    1.0000    1.0000       564\n",
      "      talk.politics.misc     1.0000    0.9978    0.9989       465\n",
      "      talk.religion.misc     1.0000    1.0000    1.0000       377\n",
      "\n",
      "               micro avg     0.9982    0.9982    0.9982     11293\n",
      "               macro avg     0.9983    0.9983    0.9983     11293\n",
      "            weighted avg     0.9982    0.9982    0.9982     11293\n",
      "\n",
      "Accuracy in testing set:0.8388468181214295\n",
      "Macro test:(0.8361540218773369, 0.8309374726105758, 0.831470097943215, None)\n",
      "Micro test:(0.8388468181214295, 0.8388468181214295, 0.8388468181214295, None)\n",
      "                          precision    recall  f1-score   support\n",
      "\n",
      "             alt.atheism     0.7864    0.7618    0.7739       319\n",
      "           comp.graphics     0.7786    0.8046    0.7914       389\n",
      " comp.os.ms-windows.misc     0.7520    0.7023    0.7263       393\n",
      "comp.sys.ibm.pc.hardware     0.6863    0.7423    0.7132       392\n",
      "   comp.sys.mac.hardware     0.7656    0.8312    0.7970       385\n",
      "          comp.windows.x     0.8912    0.7730    0.8279       392\n",
      "            misc.forsale     0.8213    0.8718    0.8458       390\n",
      "               rec.autos     0.9295    0.9013    0.9152       395\n",
      "         rec.motorcycles     0.9450    0.9497    0.9474       398\n",
      "      rec.sport.baseball     0.9280    0.9421    0.9350       397\n",
      "        rec.sport.hockey     0.9487    0.9724    0.9604       399\n",
      "               sci.crypt     0.9185    0.9394    0.9288       396\n",
      "         sci.electronics     0.7801    0.7583    0.7690       393\n",
      "                 sci.med     0.9261    0.8864    0.9058       396\n",
      "               sci.space     0.9002    0.9162    0.9082       394\n",
      "  soc.religion.christian     0.8102    0.9221    0.8625       398\n",
      "      talk.politics.guns     0.7261    0.8956    0.8020       364\n",
      "   talk.politics.mideast     0.9731    0.8644    0.9155       376\n",
      "      talk.politics.misc     0.7821    0.5903    0.6728       310\n",
      "      talk.religion.misc     0.6742    0.5936    0.6314       251\n",
      "\n",
      "               micro avg     0.8388    0.8388    0.8388      7527\n",
      "               macro avg     0.8362    0.8309    0.8315      7527\n",
      "            weighted avg     0.8408    0.8388    0.8379      7527\n",
      "Sat May  4 17:29:26 2019. At 84101 s, all done.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "svc = svm.LinearSVC()\n",
    "parameters = [{'C': [0.01, 0.1, 1, 10, 100, 1000]}]\n",
    "clf = GridSearchCV(svc, parameters, n_jobs=-1, cv=10)\n",
    "print(\"Training the classifier...\")\n",
    "clf.fit(X_train,y_train)\n",
    "forest = clf.fit(X_train, y_train)\n",
    "report = \"\"\n",
    "pred_train = forest.predict(X_train)\n",
    "\n",
    "# training score\n",
    "score = accuracy_score(y_train, pred_train)\n",
    "# score = metrics.f1_score(y_test, pred_test, pos_label=list(set(y_test)))\n",
    "acc = \"Accuracy in training set:\" + str(score)\n",
    "mac = \"Macro:\" + str(metrics.precision_recall_fscore_support(y_train, pred_train, average='macro'))\n",
    "mic = \"Micro:\" + str(metrics.precision_recall_fscore_support(y_train, pred_train, average='micro'))\n",
    "met = metrics.classification_report(y_train, pred_train, target_names=categories, digits=4)\n",
    "\n",
    "report += \"\\nFeatures shape:\" + str(X_train.shape) + \"\\n\"\n",
    "report += '\\n'.join([acc, mac, mic, met])\n",
    "\n",
    "pred_test = forest.predict(X_test)\n",
    "\n",
    "# testing score\n",
    "score = accuracy_score(y_test, pred_test)\n",
    "# score = metrics.f1_score(y_test, pred_test, pos_label=list(set(y_test)))\n",
    "acc = \"Accuracy in testing set:\" + str(score)\n",
    "mac = \"Macro test:\" + str(metrics.precision_recall_fscore_support(y_test, pred_test, average='macro'))\n",
    "mic = \"Micro test:\" + str(metrics.precision_recall_fscore_support(y_test, pred_test, average='micro'))\n",
    "met = metrics.classification_report(y_test, pred_test, target_names=categories, digits=4)\n",
    "report += '\\n'+'\\n'.join([acc, mac, mic, met])\n",
    "report += Time(\"all done.\")\n",
    "\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dustbin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(documents)\n",
    "print(vectorizer.get_feature_names())\n",
    "\n",
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = \"webkb\"\n",
    "new_collection = []\n",
    "with open(\"data/%s/%s-train-stemmed.txt\" % (corpus, corpus),'r') as f:\n",
    "    collection = f.readlines()\n",
    "    for doc in collection:\n",
    "        words = doc.split()\n",
    "        if len(words) > 1:\n",
    "            doc = ' '.join(words)+'\\n'\n",
    "            new_collection.append(doc)\n",
    "with open(\"data/%s/%s-train-stemmed.txt\" % (corpus, corpus),'w') as f:\n",
    "    f.writelines(new_collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2785"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(new_collection)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
